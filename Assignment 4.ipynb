{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 0]\n",
      "[4 3 2 0]\n",
      "[1 2 3 0]\n",
      "[4 3 2 0]\n",
      "[1 2 3 0]\n",
      "[4 3 2 0]\n",
      "[1 2 3 0]\n",
      "[4 3 2 0]\n",
      "[4 5 6 1]\n",
      "[7 8 9 1]\n",
      "[4 5 6 1]\n",
      "[7 8 9 1]\n",
      "[4 5 6 1]\n",
      "[7 8 9 1]\n",
      "[4 5 6 1]\n",
      "[7 8 9 1]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,2,3,0],[4,5,6,1],[7,8,9,1],[4,3,2,0],[1,2,3,0],[4,5,6,1],[7,8,9,1],[4,3,2,0],[1,2,3,0],[4,5,6,1],[7,8,9,1],[4,3,2,0],[1,2,3,0],[4,5,6,1],[7,8,9,1],[4,3,2,0]])\n",
    "x = sorted(x,key = lambda a:a[3])\n",
    "for i in x:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "x = np.array(x)\n",
    "i = x[:,3]\n",
    "print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>103</td>\n",
       "      <td>87</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>53</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>53</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>137</td>\n",
       "      <td>126</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>224</td>\n",
       "      <td>222</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      0       0       0       0       0       0       0       0       9   \n",
       "1      1       0       0       0       0       0       0       0       0   \n",
       "2      2       0       0       0       0       0       0      14      53   \n",
       "3      2       0       0       0       0       0       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9    ...     pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       8    ...          103        87        56         0         0   \n",
       "1       0    ...           34         0         0         0         0   \n",
       "2      99    ...            0         0         0         0        63   \n",
       "3       0    ...          137       126       140         0       133   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2        53        31         0         0         0  \n",
       "3       224       222        56         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r\"G:\\Coding Blocks\\Documents\\fashion.csv\")\n",
    "data.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(10000, 785)\n",
      "[[ 0  0  0 ...  0  0  0]\n",
      " [ 1  0  0 ...  0  0  0]\n",
      " [ 2  0  0 ...  0  0  0]\n",
      " [ 2  0  0 ... 56  0  0]\n",
      " [ 3  0  0 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "train = data.values\n",
    "print(type(train))\n",
    "print(train.shape)\n",
    "print(train[:5,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 785)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "train = sorted(train,key = lambda x:x[0])\n",
    "train = np.array(train)\n",
    "print(train.shape)\n",
    "print(train[:5,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "9\n",
      "[0 0 0 ... 9 9 9]\n"
     ]
    }
   ],
   "source": [
    "i = train[:,0]\n",
    "print(i.shape)\n",
    "print(i[-1])\n",
    "i.reshape(10000,1)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idex(i,num):\n",
    "    return list(i).index(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "z = idex(i,1)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "classes = list()\n",
    "labels = list()\n",
    "print(i[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deviding the train dataset according to labels and storing in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 784)\n",
      "(1000, 784)\n",
      "(1000, 784)\n",
      "(1000, 784)\n",
      "(1000, 784)\n",
      "(1000, 784)\n",
      "(1000, 784)\n",
      "(1000, 784)\n",
      "(1000, 784)\n",
      "(1000, 784)\n"
     ]
    }
   ],
   "source": [
    "def construct():\n",
    "    t = int()\n",
    "    prev = 0\n",
    "    for z in range(1,10):\n",
    "        t = idex(i,z)\n",
    "        temp = train[prev:t,1:]\n",
    "        temp2 = train[prev:t,0]\n",
    "        labels.append(temp2)\n",
    "        print(temp.shape)\n",
    "        classes.append(temp)\n",
    "        prev = t\n",
    "    temp = train[t:,1:]\n",
    "    temp2= train[t:,0]\n",
    "    labels.append(temp)\n",
    "    classes.append(temp)\n",
    "\n",
    "construct()\n",
    "print(classes[8].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ... 18  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(classes[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "[[0 1 2 3]\n",
      " [1 4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "t1 = np.array([[1],[2],[3]])\n",
    "print(t1.shape)\n",
    "h = np.append([[0],[1]],[[1, 2, 3], [4, 5, 6]], axis=1)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Starts Here\n",
    "- Note 1: The self made logistic regression algo is giving run-time error with reasons like : exponent value overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm\n",
    "def hypothesis(x,theta,bias):\n",
    "    result = bias + np.dot(x,theta)\n",
    "    return sigmoid(result)\n",
    "\n",
    "def sigmoid(a):\n",
    "    return 1.0/(1.0 + np.exp(-1.0*a))   # Use floats for accurate results\n",
    "\n",
    "def error(x,theta,y,bias):\n",
    "    it = x.shape[0]\n",
    "    e = 0.0\n",
    "    for i in range(it):\n",
    "        h = hypothesis(x[i],theta,bias)\n",
    "        e += y[i]*np.log2(h) + (1-y[i])*np.log2(1-h)\n",
    "    return -e/it\n",
    "\n",
    "def grad(x,y,theta,bias,batch_size = 1):\n",
    "    it = x.shape[0]\n",
    "    indices = np.arange(it)\n",
    "    np.random.shuffle(indices)\n",
    "    indices = indices[:batch_size]\n",
    "    g =  np.zeros(theta.shape)\n",
    "    g_b = 0.0\n",
    "    for i in indices:\n",
    "        h = hypothesis(x[i],theta,bias)\n",
    "        g += (y[i] - h)*x[i]\n",
    "        g_b += (y[i] - h)\n",
    "    g /= it\n",
    "    g_b /= it\n",
    "    return (g,g_b)\n",
    "\n",
    "def grad_des(x,y,theta,bias,learning_rate = 0.1):\n",
    "    e =  error(x,theta,y,bias)\n",
    "    g,g_b = grad(x,y,theta,bias)\n",
    "    theta = theta + learning_rate*g\n",
    "    bias = bias + learning_rate*g_b\n",
    "    return theta,bias\n",
    "\n",
    "def predict(x,theta,bias):\n",
    "    \n",
    "    confidence = hypothesis(x,theta,bias)\n",
    "    if confidence<0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# print(labels[1])\n",
    "print(labels[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41306601 0.47495222 0.72309103 0.72704929 0.13244985 0.15567719\n",
      " 0.05019507 0.70478325 0.31664088 0.26554966 0.55250934 0.23133162\n",
      " 0.86685887 0.22515249 0.38592032 0.30914165 0.92908369 0.13929328\n",
      " 0.48175972 0.32922921 0.44583868 0.60381108 0.02041504 0.94590398\n",
      " 0.46221281 0.52301472 0.60594276 0.5156538  0.87060536 0.95769396\n",
      " 0.15245809 0.9775288  0.00969691 0.25425469 0.71414484 0.60358479\n",
      " 0.56201058 0.36994884 0.26983378 0.80978822 0.410881   0.59589878\n",
      " 0.41641394 0.7933691  0.28714537 0.25164821 0.04471727 0.31412567\n",
      " 0.76242388 0.62180146 0.41154881 0.46571644 0.99498021 0.97090139\n",
      " 0.90277077 0.72875406 0.10047056 0.81806192 0.72369844 0.76506979\n",
      " 0.66742065 0.94265493 0.49578319 0.32794253 0.15301361 0.10273475\n",
      " 0.68779652 0.30908673 0.31980809 0.48215632 0.04111449 0.50844129\n",
      " 0.40657664 0.27275398 0.82738559 0.24966665 0.16880471 0.61993001\n",
      " 0.42379513 0.76992976 0.63524684 0.39975369 0.10852427 0.06171054\n",
      " 0.42089785 0.91533187 0.40946699 0.84636875 0.4385158  0.60167309\n",
      " 0.02770461 0.85656095 0.4011904  0.56927744 0.2940369  0.6440908\n",
      " 0.6798382  0.75888147 0.06375215 0.34331927 0.97702235 0.31167658\n",
      " 0.23840549 0.88920824 0.45453005 0.46463919 0.3863843  0.14490321\n",
      " 0.68457074 0.37976163 0.53489942 0.1997608  0.39002468 0.22283028\n",
      " 0.56542169 0.81286658 0.07302984 0.71528163 0.9232362  0.30824107\n",
      " 0.56499283 0.18199185 0.20019774 0.13347474 0.25580939 0.66880293\n",
      " 0.28549306 0.85604448 0.64831269 0.73652691 0.83627911 0.65209846\n",
      " 0.47706238 0.91423789 0.38463008 0.0116747  0.20675196 0.54467367\n",
      " 0.06869782 0.28761219 0.24975292 0.89935436 0.32976476 0.17871822\n",
      " 0.8755199  0.08409112 0.77227116 0.57352961 0.03353065 0.05665455\n",
      " 0.82080226 0.09841942 0.31935566 0.961127   0.17872493 0.23924477\n",
      " 0.07393239 0.93350582 0.98080173 0.02310585 0.55685133 0.31470735\n",
      " 0.18965837 0.12260852 0.63726114 0.26464259 0.48921714 0.64226148\n",
      " 0.99358621 0.92889562 0.90333533 0.89546153 0.08393327 0.768353\n",
      " 0.88183081 0.19293187 0.23317848 0.53965296 0.3335392  0.98067617\n",
      " 0.56088398 0.12309797 0.58053995 0.74034435 0.29177634 0.36064878\n",
      " 0.23170449 0.01698038 0.95441463 0.73390306 0.83828789 0.47162334\n",
      " 0.94623829 0.5698494  0.74117366 0.90009082 0.75452445 0.76876424\n",
      " 0.87954645 0.96244841 0.94959688 0.27081464 0.73429378 0.46048569\n",
      " 0.41557466 0.31241483 0.96019123 0.1324435  0.65008028 0.24109639\n",
      " 0.46640124 0.34161457 0.13415612 0.32560706 0.04807744 0.36647688\n",
      " 0.89050671 0.19809597 0.01005505 0.78873273 0.06254062 0.94989092\n",
      " 0.3950335  0.21150206 0.35636298 0.76277316 0.59610075 0.13322342\n",
      " 0.73337343 0.73557998 0.40994667 0.41887901 0.78774525 0.47557143\n",
      " 0.86161062 0.8990469  0.68765855 0.85595412 0.97539364 0.11335115\n",
      " 0.65532445 0.69871535 0.94461984 0.07651025 0.51823885 0.05592351\n",
      " 0.51433475 0.58218958 0.81446223 0.67576079 0.70315651 0.47396372\n",
      " 0.3430639  0.57232968 0.52635119 0.88951115 0.69330792 0.02390379\n",
      " 0.91054146 0.10623229 0.44488458 0.22661687 0.9659814  0.93797186\n",
      " 0.41439123 0.73042563 0.55265275 0.79814927 0.67424301 0.04687282\n",
      " 0.2770526  0.79590622 0.04747067 0.96879878 0.60308983 0.09349017\n",
      " 0.19504115 0.28701213 0.6129134  0.2636103  0.37236892 0.64109646\n",
      " 0.24254279 0.68411085 0.96773691 0.02125993 0.77718265 0.05054712\n",
      " 0.6289693  0.7982377  0.68822224 0.39986734 0.2281396  0.4905512\n",
      " 0.24796177 0.92391765 0.01094069 0.92191405 0.31963972 0.61481221\n",
      " 0.07392774 0.85438342 0.67536585 0.59759474 0.24083875 0.06882254\n",
      " 0.70758862 0.73060037 0.16233191 0.37136763 0.18699158 0.65589826\n",
      " 0.66404163 0.19318741 0.33373467 0.53047202 0.96558169 0.96827404\n",
      " 0.05435885 0.74356031 0.43459644 0.02260935 0.94427468 0.34145699\n",
      " 0.61175262 0.70395861 0.42953262 0.84542167 0.44074283 0.2765269\n",
      " 0.39750599 0.80319404 0.30703491 0.25883422 0.67143551 0.27063233\n",
      " 0.12396543 0.6583302  0.64467906 0.01061556 0.53156776 0.57928814\n",
      " 0.47516883 0.11113689 0.59514026 0.40367335 0.67219131 0.38581042\n",
      " 0.01669906 0.39978872 0.78132759 0.03858109 0.78924042 0.68356251\n",
      " 0.25530872 0.3137092  0.30332974 0.59499544 0.40208633 0.50309342\n",
      " 0.64139973 0.27735289 0.26613398 0.96707386 0.06825547 0.18243807\n",
      " 0.96096871 0.44079933 0.42867877 0.30538025 0.00786713 0.53381177\n",
      " 0.85050499 0.52982863 0.11796245 0.30691086 0.21542091 0.40387831\n",
      " 0.64527825 0.71965377 0.73276384 0.1415674  0.31033869 0.20146695\n",
      " 0.87842941 0.10510069 0.26486768 0.97639727 0.05133738 0.86587569\n",
      " 0.7601675  0.12162575 0.97752217 0.91754341 0.35006193 0.27245425\n",
      " 0.24901706 0.29081415 0.79066962 0.77166652 0.58272878 0.57644296\n",
      " 0.32950732 0.81616229 0.91898978 0.75514523 0.14106391 0.3122141\n",
      " 0.92661389 0.61588281 0.95331407 0.03448747 0.90742881 0.25663671\n",
      " 0.17398353 0.75482552 0.5070321  0.52862623 0.09219041 0.86514882\n",
      " 0.64593249 0.75406558 0.97958656 0.92351173 0.56949595 0.42973031\n",
      " 0.19360309 0.18891512 0.10162577 0.72310651 0.00797246 0.78365441\n",
      " 0.9020228  0.3984729  0.02772909 0.55955768 0.60223269 0.26268271\n",
      " 0.41890255 0.84633328 0.15709427 0.42042275 0.44645895 0.92336642\n",
      " 0.10504315 0.53833014 0.07398694 0.61780695 0.71134335 0.72654863\n",
      " 0.02527178 0.50040577 0.45646508 0.61798773 0.81570863 0.27931251\n",
      " 0.00643692 0.20360286 0.00696255 0.97504807 0.26273462 0.33631332\n",
      " 0.9901552  0.03104388 0.57275001 0.93962705 0.33745499 0.17671459\n",
      " 0.4258558  0.82986891 0.36252653 0.60306279 0.82409899 0.15724526\n",
      " 0.07778754 0.89801019 0.19202135 0.88007931 0.99795384 0.23134713\n",
      " 0.7275823  0.74807326 0.37289175 0.73124194 0.49452334 0.05236891\n",
      " 0.13017495 0.08139011 0.58292559 0.80635852 0.18831186 0.17115869\n",
      " 0.18128979 0.85731077 0.10464294 0.65720781 0.73534313 0.44988487\n",
      " 0.03444716 0.18039771 0.94190133 0.84122071 0.59571641 0.11571531\n",
      " 0.62362762 0.05107441 0.80890774 0.70234659 0.14419181 0.62210981\n",
      " 0.48231784 0.11728678 0.52638173 0.18932358 0.7348497  0.95095033\n",
      " 0.5921584  0.94806547 0.70942735 0.57714576 0.0109406  0.92324018\n",
      " 0.41492514 0.44490901 0.66012108 0.16235933 0.05184478 0.95686824\n",
      " 0.79504448 0.52051558 0.96247939 0.08588801 0.86352763 0.98357184\n",
      " 0.52298276 0.40272262 0.57629804 0.53791945 0.9457817  0.47371412\n",
      " 0.76254159 0.98129216 0.02756896 0.85451933 0.28577171 0.36043091\n",
      " 0.25479056 0.87034065 0.99176096 0.07056489 0.40416868 0.61242168\n",
      " 0.27786473 0.19645969 0.93333178 0.07655523 0.61340339 0.48709932\n",
      " 0.17435605 0.56756977 0.62883207 0.57815159 0.26259585 0.32917269\n",
      " 0.69894661 0.80643661 0.41797491 0.36294984 0.01283341 0.72017594\n",
      " 0.52031205 0.4025148  0.16281185 0.62921116 0.12724639 0.840801\n",
      " 0.52475067 0.93530309 0.35847528 0.2071408  0.15605133 0.66675327\n",
      " 0.39058775 0.53802532 0.48539674 0.59702744 0.9796191  0.14291405\n",
      " 0.15163315 0.9155308  0.87150086 0.62323131 0.07559423 0.81258597\n",
      " 0.46070221 0.00514463 0.07039534 0.16008233 0.96649075 0.50283116\n",
      " 0.03870156 0.84078941 0.58659182 0.52778901 0.6801753  0.34531817\n",
      " 0.16728053 0.46552179 0.96618492 0.00323422 0.5214867  0.36525116\n",
      " 0.22159234 0.37534096 0.36875466 0.95840502 0.52045546 0.23825786\n",
      " 0.46787474 0.86698279 0.68308163 0.22851129 0.59987858 0.00331487\n",
      " 0.01354002 0.27805175 0.84632645 0.63669019 0.97778397 0.16964916\n",
      " 0.02549748 0.47904685 0.8542243  0.16719149 0.05305099 0.46339649\n",
      " 0.08452172 0.02031228 0.03854753 0.76914767 0.04431546 0.30254065\n",
      " 0.04169914 0.28380129 0.17498293 0.80245623 0.28776404 0.36892037\n",
      " 0.14511925 0.24284863 0.84571441 0.80699286 0.45919092 0.89741581\n",
      " 0.11221624 0.21119147 0.48826726 0.05294034 0.98771247 0.01861068\n",
      " 0.91011597 0.6607033  0.06998178 0.45903513 0.94760004 0.36438622\n",
      " 0.66231972 0.69677928 0.65782081 0.32286543 0.48147869 0.75924241\n",
      " 0.11364579 0.17759324 0.78511988 0.35834819 0.36135914 0.81645837\n",
      " 0.02949285 0.50042517 0.04376043 0.84467747 0.8573741  0.52585414\n",
      " 0.63488482 0.57154883 0.7858982  0.22474864 0.06126905 0.25371163\n",
      " 0.10462145 0.5297368  0.09009912 0.74787293 0.17168394 0.07876408\n",
      " 0.41855177 0.76658021 0.1436456  0.34154776 0.78618096 0.0297098\n",
      " 0.00115587 0.73490585 0.93028573 0.5308668  0.56995633 0.057049\n",
      " 0.68904004 0.78967812 0.90248506 0.87769493 0.28575222 0.93508755\n",
      " 0.34312593 0.3734694  0.44171032 0.70738791 0.33346354 0.08939985\n",
      " 0.58405914 0.57073291 0.21049251 0.12268127 0.68501671 0.67296855\n",
      " 0.97246058 0.86919371 0.3598289  0.84510922 0.2236801  0.67998157\n",
      " 0.14380924 0.98436796 0.14057135 0.87827738 0.66345598 0.32918073\n",
      " 0.03831553 0.42756406 0.28149677 0.32514858 0.13013366 0.80829443\n",
      " 0.00555955 0.03953882 0.32241416 0.11362407 0.57694805 0.1993602\n",
      " 0.45563102 0.27669046 0.69957054 0.42871719 0.87290806 0.49703404\n",
      " 0.3362921  0.81867408 0.5727141  0.17495245 0.5571733  0.04972206\n",
      " 0.94723933 0.89341212 0.29550501 0.6985047  0.7568732  0.28899862\n",
      " 0.76133394 0.14517406 0.39876707 0.0667004  0.49475881 0.08436131\n",
      " 0.02631869 0.86297357 0.78032176 0.13445498 0.93347041 0.58305422\n",
      " 0.27878112 0.69201991 0.4858819  0.04808492]\n"
     ]
    }
   ],
   "source": [
    "theta1 = np.random.random((classes[0].shape[1],))\n",
    "print(theta1)\n",
    "bias1 = np.random.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for labels 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.concatenate((classes[5],classes[6]))\n",
    "y1 = np.concatenate((labels[0],labels[1]))\n",
    "# for i in range(2000):\n",
    "#     t,b = grad_des(x1,y1,theta1,bias1)\n",
    "#     theta1 = t\n",
    "#     bias1 = b\n",
    "    \n",
    "# print(theta1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(x1,y1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest = x1[1005,:].reshape(1,-1)\n",
    "lr.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for labels 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.concatenate((classes[1],classes[2]))\n",
    "y1 = np.concatenate((labels[0],labels[1]))\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x1,y1)\n",
    "lr.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for labels 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.concatenate((classes[2],classes[3]))\n",
    "y1 = np.concatenate((labels[0],labels[1]))\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x1,y1)\n",
    "lr.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for labels 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.concatenate((classes[3],classes[4]))\n",
    "y1 = np.concatenate((labels[0],labels[1]))\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x1,y1)\n",
    "lr.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {0:0,1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0}\n",
    "op = 0\n",
    "for i in range(9):\n",
    "    for j in range(i+1,10):\n",
    "        op+=1\n",
    "        x1 = np.concatenate((classes[i],classes[j]))\n",
    "        y1 = np.concatenate((labels[0],labels[1]))\n",
    "        lr = LogisticRegression()\n",
    "        lr.fit(x1,y1)\n",
    "        t = lr.predict(xtest)\n",
    "        if t[0] == 0:\n",
    "            result[i]+=1\n",
    "        else:\n",
    "            result[j]+=1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 8, 1: 4, 2: 7, 3: 5, 4: 3, 5: 2, 6: 9, 7: 0, 8: 6, 9: 1}\n",
      "Number of combinations checked = 45\n"
     ]
    }
   ],
   "source": [
    "print(result)\n",
    "print(\"Number of combinations checked = {0}\".format(op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "max_label = max(result,key = lambda x:result[x])\n",
    "print(max_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output predicted is right in the case shown above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
