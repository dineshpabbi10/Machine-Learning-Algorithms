{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 0]\n",
      "[4 3 2 0]\n",
      "[1 2 3 0]\n",
      "[4 3 2 0]\n",
      "[1 2 3 0]\n",
      "[4 3 2 0]\n",
      "[1 2 3 0]\n",
      "[4 3 2 0]\n",
      "[4 5 6 1]\n",
      "[7 8 9 1]\n",
      "[4 5 6 1]\n",
      "[7 8 9 1]\n",
      "[4 5 6 1]\n",
      "[7 8 9 1]\n",
      "[4 5 6 1]\n",
      "[7 8 9 1]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,2,3,0],[4,5,6,1],[7,8,9,1],[4,3,2,0],[1,2,3,0],[4,5,6,1],[7,8,9,1],[4,3,2,0],[1,2,3,0],[4,5,6,1],[7,8,9,1],[4,3,2,0],[1,2,3,0],[4,5,6,1],[7,8,9,1],[4,3,2,0]])\n",
    "x = sorted(x,key = lambda a:a[3])\n",
    "for i in x:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "x = np.array(x)\n",
    "i = x[:,3]\n",
    "print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>103</td>\n",
       "      <td>87</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>53</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>53</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>137</td>\n",
       "      <td>126</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>224</td>\n",
       "      <td>222</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      0       0       0       0       0       0       0       0       9   \n",
       "1      1       0       0       0       0       0       0       0       0   \n",
       "2      2       0       0       0       0       0       0      14      53   \n",
       "3      2       0       0       0       0       0       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9    ...     pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       8    ...          103        87        56         0         0   \n",
       "1       0    ...           34         0         0         0         0   \n",
       "2      99    ...            0         0         0         0        63   \n",
       "3       0    ...          137       126       140         0       133   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2        53        31         0         0         0  \n",
       "3       224       222        56         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r\"G:\\Coding Blocks\\Documents\\fashion.csv\")\n",
    "data.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(10000, 785)\n",
      "[[ 0  0  0 ...  0  0  0]\n",
      " [ 1  0  0 ...  0  0  0]\n",
      " [ 2  0  0 ...  0  0  0]\n",
      " [ 2  0  0 ... 56  0  0]\n",
      " [ 3  0  0 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "train = data.values\n",
    "print(type(train))\n",
    "print(train.shape)\n",
    "print(train[:5,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 785)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "train = sorted(train,key = lambda x:x[0])\n",
    "train = np.array(train)\n",
    "print(train.shape)\n",
    "print(train[:5,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "9\n",
      "[0 0 0 ... 9 9 9]\n"
     ]
    }
   ],
   "source": [
    "i = train[:,0]\n",
    "print(i.shape)\n",
    "print(i[-1])\n",
    "i.reshape(10000,1)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idex(i,num):\n",
    "    return list(i).index(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "z = idex(i,1)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "classes = list()\n",
    "labels = list()\n",
    "print(i[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deviding the train dataset according to labels and storing in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 784)\n",
      "(1000, 784)\n",
      "(1000, 784)\n",
      "(1000, 784)\n",
      "(1000, 784)\n",
      "(1000, 784)\n",
      "(1000, 784)\n",
      "(1000, 784)\n",
      "(1000, 784)\n",
      "(1000, 784)\n"
     ]
    }
   ],
   "source": [
    "def construct():\n",
    "    t = int()\n",
    "    prev = 0\n",
    "    for z in range(1,10):\n",
    "        t = idex(i,z)\n",
    "        temp = train[prev:t,1:]\n",
    "        temp2 = train[prev:t,0]\n",
    "        labels.append(temp2)\n",
    "        print(temp.shape)\n",
    "        classes.append(temp)\n",
    "        prev = t\n",
    "    temp = train[t:,1:]\n",
    "    temp2= train[t:,0]\n",
    "    labels.append(temp)\n",
    "    classes.append(temp)\n",
    "\n",
    "construct()\n",
    "print(classes[8].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ... 18  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(classes[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "[[0 1 2 3]\n",
      " [1 4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "t1 = np.array([[1],[2],[3]])\n",
    "print(t1.shape)\n",
    "h = np.append([[0],[1]],[[1, 2, 3], [4, 5, 6]], axis=1)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Starts Here\n",
    "- Note 1: The self made logistic regression algo is giving run-time error with reasons like : exponent value overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm\n",
    "def hypothesis(x,theta,bias):\n",
    "    result = bias + np.dot(x,theta)\n",
    "    return sigmoid(result)\n",
    "\n",
    "def sigmoid(a):\n",
    "    return 1.0/(1.0 + np.exp(-1.0*a))   # Use floats for accurate results\n",
    "\n",
    "def error(x,theta,y,bias):\n",
    "    it = x.shape[0]\n",
    "    e = 0.0\n",
    "    for i in range(it):\n",
    "        h = hypothesis(x[i],theta,bias)\n",
    "        e += y[i]*np.log2(h) + (1-y[i])*np.log2(1-h)\n",
    "    return -e/it\n",
    "\n",
    "def grad(x,y,theta,bias,batch_size = 1):\n",
    "    it = x.shape[0]\n",
    "    indices = np.arange(it)\n",
    "    np.random.shuffle(indices)\n",
    "    indices = indices[:batch_size]\n",
    "    g =  np.zeros(theta.shape)\n",
    "    g_b = 0.0\n",
    "    for i in indices:\n",
    "        h = hypothesis(x[i],theta,bias)\n",
    "        g += (y[i] - h)*x[i]\n",
    "        g_b += (y[i] - h)\n",
    "    g /= it\n",
    "    g_b /= it\n",
    "    return (g,g_b)\n",
    "\n",
    "def grad_des(x,y,theta,bias,learning_rate = 0.1):\n",
    "    e =  error(x,theta,y,bias)\n",
    "    g,g_b = grad(x,y,theta,bias)\n",
    "    theta = theta + learning_rate*g\n",
    "    bias = bias + learning_rate*g_b\n",
    "    return theta,bias\n",
    "\n",
    "def predict(x,theta,bias):\n",
    "    \n",
    "    confidence = hypothesis(x,theta,bias)\n",
    "    if confidence<0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# print(labels[1])\n",
    "print(labels[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10055399 0.65979212 0.56288563 0.03868751 0.40500411 0.52774836\n",
      " 0.90952623 0.60902317 0.44042246 0.35228647 0.23809964 0.83175538\n",
      " 0.76079792 0.65249102 0.77859318 0.82554385 0.08760599 0.44782671\n",
      " 0.24870423 0.11171974 0.05644657 0.91511332 0.25932164 0.60482642\n",
      " 0.95980117 0.40337086 0.28001152 0.59582329 0.37892098 0.01045775\n",
      " 0.3179218  0.91031908 0.84615876 0.43627448 0.50908655 0.06924123\n",
      " 0.4020001  0.20557833 0.92490045 0.47501873 0.85402762 0.97318038\n",
      " 0.8044622  0.50431006 0.88048611 0.592924   0.89664275 0.53199249\n",
      " 0.43074751 0.99068162 0.47061067 0.71709493 0.38684101 0.12337848\n",
      " 0.69153924 0.01241124 0.93106193 0.9347429  0.99713499 0.78229242\n",
      " 0.96622409 0.92165525 0.95638822 0.65682596 0.09669044 0.82563354\n",
      " 0.58058752 0.40651867 0.90564113 0.07752342 0.36606484 0.08563744\n",
      " 0.86271982 0.68633129 0.13896292 0.31041668 0.94029853 0.26876979\n",
      " 0.8311865  0.31141499 0.40819874 0.83740935 0.153213   0.30248779\n",
      " 0.79524362 0.56695285 0.45105948 0.17751885 0.45176863 0.48130614\n",
      " 0.27301736 0.98781322 0.1788377  0.93083052 0.22279558 0.85048105\n",
      " 0.81876763 0.5072164  0.09793461 0.42122034 0.86475009 0.19275922\n",
      " 0.54803899 0.10167564 0.48873782 0.23743722 0.34411976 0.38948065\n",
      " 0.84224456 0.40708614 0.87728862 0.21502075 0.23865038 0.18174176\n",
      " 0.78595962 0.54611244 0.96657614 0.76768604 0.14122386 0.95366226\n",
      " 0.39325146 0.13659207 0.02537851 0.45873406 0.6638245  0.45251041\n",
      " 0.42749424 0.02482413 0.11084339 0.09255605 0.95951596 0.73706367\n",
      " 0.56021854 0.0215028  0.13685956 0.26379027 0.90624429 0.20778601\n",
      " 0.61642258 0.07349105 0.09474066 0.86187172 0.54727506 0.49356852\n",
      " 0.68708204 0.05381727 0.05567484 0.21170708 0.31889618 0.907164\n",
      " 0.23160197 0.28130298 0.35255233 0.13933026 0.91883962 0.3986816\n",
      " 0.51011452 0.66039454 0.74033744 0.74064478 0.21120905 0.64007904\n",
      " 0.26886163 0.4353626  0.50299902 0.68384755 0.27964463 0.42760465\n",
      " 0.75076438 0.90201055 0.98620094 0.91726573 0.94916481 0.61808513\n",
      " 0.62752644 0.84702086 0.76619008 0.72645966 0.98565351 0.03608897\n",
      " 0.39578301 0.81293216 0.41541374 0.4370835  0.56685289 0.21656131\n",
      " 0.77749788 0.37641337 0.67773314 0.58311073 0.49514242 0.0896924\n",
      " 0.85027711 0.41512949 0.91467031 0.27174221 0.55932031 0.26921888\n",
      " 0.03315358 0.64671001 0.42252855 0.26909391 0.09822646 0.43707623\n",
      " 0.7326574  0.76179246 0.67048681 0.04073382 0.82635474 0.8490954\n",
      " 0.62526818 0.49006271 0.3600666  0.32599042 0.35200054 0.45878053\n",
      " 0.25740724 0.99862384 0.32240712 0.13729212 0.6688525  0.7123001\n",
      " 0.79092314 0.16093433 0.5249356  0.98383044 0.28798738 0.13805712\n",
      " 0.51414882 0.11852286 0.17089559 0.00181472 0.85286323 0.84070878\n",
      " 0.84406067 0.86420349 0.4705667  0.19773511 0.7778303  0.28893336\n",
      " 0.94206111 0.75851977 0.87407407 0.42875575 0.15618697 0.44907937\n",
      " 0.4052094  0.9725058  0.71938911 0.34212863 0.85374025 0.86886776\n",
      " 0.52341015 0.07537438 0.0289921  0.19479892 0.1146239  0.02699768\n",
      " 0.47295635 0.18682336 0.13234701 0.46399749 0.12502077 0.81816295\n",
      " 0.2828233  0.33112332 0.41112191 0.70750594 0.29870911 0.7867518\n",
      " 0.32973977 0.11653829 0.09867442 0.1154368  0.00436384 0.96848094\n",
      " 0.01275895 0.47106897 0.16646888 0.63720612 0.24397396 0.8229143\n",
      " 0.46230706 0.86802147 0.38494523 0.13139873 0.13552027 0.530692\n",
      " 0.93553848 0.84921616 0.50460379 0.92955103 0.94162336 0.87367299\n",
      " 0.59109049 0.99382001 0.04041932 0.58510769 0.76853065 0.13216486\n",
      " 0.7274602  0.01800727 0.36333133 0.25597232 0.37993438 0.23081752\n",
      " 0.8313985  0.65798556 0.9050858  0.18543082 0.56512354 0.78065603\n",
      " 0.135969   0.82839559 0.55959034 0.65445182 0.44192195 0.69287971\n",
      " 0.07023098 0.59419945 0.16921773 0.55303354 0.10188355 0.40957567\n",
      " 0.05925513 0.18901446 0.78943052 0.29538669 0.84168455 0.74767331\n",
      " 0.9701807  0.01804574 0.56191877 0.0412335  0.02667463 0.11241227\n",
      " 0.19966632 0.69739505 0.92510573 0.57241619 0.75977462 0.5846637\n",
      " 0.3141283  0.46937446 0.28995043 0.09053758 0.2724904  0.93218602\n",
      " 0.24242371 0.46540782 0.15688075 0.24612203 0.42591294 0.57781089\n",
      " 0.24801826 0.03122868 0.92550712 0.44518601 0.24171792 0.13834351\n",
      " 0.79287648 0.52768661 0.67386237 0.96868301 0.80428505 0.52683248\n",
      " 0.91206289 0.41379241 0.47088415 0.53161505 0.84406503 0.78972414\n",
      " 0.92079684 0.05300685 0.78239262 0.70891246 0.61451584 0.13190199\n",
      " 0.63434422 0.93690651 0.55788412 0.38312638 0.21406486 0.99603999\n",
      " 0.87364698 0.74982357 0.56147233 0.49453561 0.80577685 0.14299362\n",
      " 0.95408024 0.35297066 0.46763987 0.87067768 0.09123153 0.95195002\n",
      " 0.20533214 0.29313814 0.879038   0.35856247 0.42085258 0.96232581\n",
      " 0.60612839 0.07746757 0.18602797 0.99381968 0.67441664 0.54184239\n",
      " 0.25499727 0.45527208 0.96161181 0.76637606 0.77755043 0.20541097\n",
      " 0.50695618 0.94042749 0.43657175 0.53773807 0.43107003 0.56781451\n",
      " 0.55427106 0.07188461 0.74881066 0.18995984 0.69006755 0.86941108\n",
      " 0.41935587 0.14968113 0.10747362 0.37733227 0.81585482 0.25097437\n",
      " 0.27733301 0.96332448 0.82134724 0.74426563 0.74426856 0.25941175\n",
      " 0.27766265 0.32408258 0.03395507 0.12708606 0.47862878 0.70310708\n",
      " 0.15237404 0.08242483 0.2540858  0.02153051 0.67393694 0.47467852\n",
      " 0.47127868 0.49278498 0.43606467 0.84353278 0.66879561 0.62261604\n",
      " 0.97988996 0.6919486  0.06583344 0.5499264  0.25424149 0.99245765\n",
      " 0.87272442 0.22133922 0.22034821 0.70329796 0.67877143 0.7782645\n",
      " 0.35666559 0.48724362 0.95547559 0.98893883 0.59379756 0.43893836\n",
      " 0.11961768 0.76481512 0.52937591 0.32204299 0.60375575 0.34343263\n",
      " 0.07452422 0.0772471  0.54907174 0.4417089  0.18416022 0.66167122\n",
      " 0.17965402 0.32717675 0.03183252 0.15664884 0.25368768 0.78388477\n",
      " 0.79055141 0.30402321 0.05010845 0.59071386 0.50656827 0.52620057\n",
      " 0.57989342 0.22688781 0.23631914 0.76565372 0.88865252 0.31676252\n",
      " 0.17871379 0.08542583 0.10082293 0.47790908 0.30733853 0.93043161\n",
      " 0.15934964 0.1249726  0.84004263 0.44036179 0.11873194 0.88769099\n",
      " 0.07111655 0.63975312 0.94469667 0.15315379 0.53774755 0.72681564\n",
      " 0.8384938  0.09438043 0.0996608  0.9607528  0.96900302 0.58544865\n",
      " 0.39395678 0.26432632 0.43249443 0.11337961 0.62498838 0.33808761\n",
      " 0.14854469 0.96905591 0.85137602 0.70547799 0.50978016 0.32027927\n",
      " 0.64223149 0.39885306 0.27568944 0.76656037 0.79098233 0.51179066\n",
      " 0.29151843 0.25473054 0.38167985 0.4504467  0.55739733 0.2529081\n",
      " 0.94036742 0.16990936 0.68587997 0.47379955 0.20933659 0.17744229\n",
      " 0.78513184 0.94820965 0.8518693  0.49698118 0.17547028 0.07349742\n",
      " 0.464224   0.68393372 0.79423867 0.35522122 0.91104902 0.03433052\n",
      " 0.93715753 0.97031623 0.04582268 0.39445389 0.76564254 0.43561929\n",
      " 0.81147378 0.20854575 0.52564933 0.63283891 0.40349759 0.90768489\n",
      " 0.66976868 0.90249032 0.32630468 0.8917482  0.46080832 0.69250652\n",
      " 0.08328035 0.53296291 0.18206654 0.68159922 0.70354453 0.38562887\n",
      " 0.4030735  0.58476285 0.08041145 0.89303187 0.53776363 0.30219028\n",
      " 0.56775817 0.94921737 0.49223833 0.89660558 0.37805323 0.02652899\n",
      " 0.69871618 0.01759876 0.21913021 0.93940576 0.97589089 0.1971876\n",
      " 0.89874522 0.12232052 0.45804869 0.12085476 0.60840195 0.19129611\n",
      " 0.12379335 0.40219343 0.03646916 0.96996414 0.430912   0.71036889\n",
      " 0.0429247  0.27064177 0.46183959 0.53959746 0.38504363 0.18296382\n",
      " 0.37777483 0.14976435 0.22956914 0.63536596 0.64704554 0.99253111\n",
      " 0.86637489 0.40143911 0.89202672 0.47888108 0.66632203 0.91807406\n",
      " 0.87074211 0.73193495 0.64660004 0.51913893 0.28271577 0.53661849\n",
      " 0.97950292 0.6916415  0.54370875 0.94866916 0.8586762  0.87341001\n",
      " 0.18643486 0.58183592 0.21104635 0.90507676 0.78765132 0.65666955\n",
      " 0.83163599 0.17580322 0.02214259 0.98986235 0.28768248 0.37418705\n",
      " 0.15265781 0.79067314 0.32967519 0.7165078  0.19130369 0.06720982\n",
      " 0.54393    0.94968216 0.89691699 0.55400407 0.68709351 0.43766963\n",
      " 0.4225078  0.54764233 0.52213387 0.44330247 0.17255341 0.08152211\n",
      " 0.88664583 0.70970395 0.14962334 0.08422931 0.63497282 0.64381434\n",
      " 0.03802121 0.70907754 0.98818493 0.93070874 0.53339035 0.67100964\n",
      " 0.84585985 0.68650477 0.48408027 0.68597579 0.39487866 0.96997554\n",
      " 0.77344613 0.28010887 0.06391758 0.12912261 0.23616793 0.3595629\n",
      " 0.52886859 0.51438914 0.70521731 0.09035926 0.72499771 0.24983006\n",
      " 0.23592777 0.51127802 0.36726861 0.41247768 0.7979727  0.40821258\n",
      " 0.57289748 0.57137421 0.03357594 0.00115612 0.9822834  0.55931287\n",
      " 0.43584919 0.1262327  0.69418345 0.40457633 0.02835572 0.25620476\n",
      " 0.61505466 0.83317753 0.84735322 0.50557401 0.68213133 0.33866241\n",
      " 0.29771723 0.49329536 0.4072724  0.29822875 0.21200943 0.23711223\n",
      " 0.49859293 0.95483622 0.05363719 0.92723849 0.78761631 0.23834085\n",
      " 0.88669752 0.96371769 0.74087576 0.52919374 0.15236733 0.74185879\n",
      " 0.17214928 0.4553668  0.97961291 0.08232463 0.85802608 0.68505621\n",
      " 0.99882535 0.56855597 0.17926618 0.69158074 0.32926448 0.13507509\n",
      " 0.37321497 0.15776262 0.88637733 0.95802114 0.5671893  0.49745773\n",
      " 0.00663396 0.93563151 0.11902872 0.66162652 0.72694424 0.65182647\n",
      " 0.46609648 0.74922778 0.20424398 0.83263842]\n"
     ]
    }
   ],
   "source": [
    "theta1 = np.random.random((classes[0].shape[1],))\n",
    "print(theta1)\n",
    "bias1 = np.random.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for labels 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10055399 0.65979212 0.56288563 0.03868751 0.40500411 0.52774836\n",
      " 0.90952623 0.60902317 0.44042246 0.35228647 0.23809964 0.83175538\n",
      " 0.76079792 0.65249102 0.77859318 0.82554385 0.08760599 0.44782671\n",
      " 0.24870423 0.11171974 0.05644657 0.91511332 0.25932164 0.60482642\n",
      " 0.95980117 0.40337086 0.28001152 0.59582329 0.37892098 0.01045775\n",
      " 0.3179218  0.91031908 0.84615876 0.43627448 0.50908655 0.06924123\n",
      " 0.4020001  0.20557833 0.92490045 0.47501873 0.85402762 0.97318038\n",
      " 0.8044622  0.50431006 0.88048611 0.592924   0.89664275 0.53199249\n",
      " 0.43074751 0.99068162 0.47061067 0.71709493 0.38684101 0.12337848\n",
      " 0.69153924 0.01241124 0.93106193 0.9347429  0.99713499 0.78229242\n",
      " 0.96622409 0.92165525 0.95638822 0.65682596 0.09669044 0.82563354\n",
      " 0.58058752 0.40651867 0.90564113 0.07752342 0.36606484 0.08563744\n",
      " 0.86271982 0.68633129 0.13896292 0.31041668 0.94029853 0.26876979\n",
      " 0.8311865  0.31141499 0.40819874 0.83740935 0.153213   0.30248779\n",
      " 0.79524362 0.56695285 0.45105948 0.17751885 0.45176863 0.48130614\n",
      " 0.27301736 0.98781322 0.1788377  0.93083052 0.22279558 0.85048105\n",
      " 0.81876763 0.5072164  0.09793461 0.42122034 0.86475009 0.19275922\n",
      " 0.54803899 0.10167564 0.48873782 0.23743722 0.34411976 0.38948065\n",
      " 0.84224456 0.40708614 0.87728862 0.21502075 0.23865038 0.18174176\n",
      " 0.78595962 0.54611244 0.96657614 0.76768604 0.14122386 0.95366226\n",
      " 0.39325146 0.13659207 0.02537851 0.45873406 0.6638245  0.45251041\n",
      " 0.42749424 0.02482413 0.11084339 0.09255605 0.95951596 0.73706367\n",
      " 0.56021854 0.0215028  0.13685956 0.26379027 0.90624429 0.20778601\n",
      " 0.61642258 0.07349105 0.09474066 0.86187172 0.54727506 0.49356852\n",
      " 0.68708204 0.05381727 0.05567484 0.21170708 0.31889618 0.907164\n",
      " 0.23160197 0.28130298 0.35255233 0.13933026 0.91883962 0.3986816\n",
      " 0.51011452 0.66039454 0.74033744 0.74064478 0.21120905 0.64007904\n",
      " 0.26886163 0.4353626  0.50299902 0.68384755 0.27964463 0.42760465\n",
      " 0.75076438 0.90201055 0.98620094 0.91726573 0.94916481 0.61808513\n",
      " 0.62752644 0.84702086 0.76619008 0.72645966 0.98565351 0.03608897\n",
      " 0.39578301 0.81293216 0.41541374 0.4370835  0.56685289 0.21656131\n",
      " 0.77749788 0.37641337 0.67773314 0.58311073 0.49514242 0.0896924\n",
      " 0.85027711 0.41512949 0.91467031 0.27174221 0.55932031 0.26921888\n",
      " 0.03315358 0.64671001 0.42252855 0.26909391 0.09822646 0.43707623\n",
      " 0.7326574  0.76179246 0.67048681 0.04073382 0.82635474 0.8490954\n",
      " 0.62526818 0.49006271 0.3600666  0.32599042 0.35200054 0.45878053\n",
      " 0.25740724 0.99862384 0.32240712 0.13729212 0.6688525  0.7123001\n",
      " 0.79092314 0.16093433 0.5249356  0.98383044 0.28798738 0.13805712\n",
      " 0.51414882 0.11852286 0.17089559 0.00181472 0.85286323 0.84070878\n",
      " 0.84406067 0.86420349 0.4705667  0.19773511 0.7778303  0.28893336\n",
      " 0.94206111 0.75851977 0.87407407 0.42875575 0.15618697 0.44907937\n",
      " 0.4052094  0.9725058  0.71938911 0.34212863 0.85374025 0.86886776\n",
      " 0.52341015 0.07537438 0.0289921  0.19479892 0.1146239  0.02699768\n",
      " 0.47295635 0.18682336 0.13234701 0.46399749 0.12502077 0.81816295\n",
      " 0.2828233  0.33112332 0.41112191 0.70750594 0.29870911 0.7867518\n",
      " 0.32973977 0.11653829 0.09867442 0.1154368  0.00436384 0.96848094\n",
      " 0.01275895 0.47106897 0.16646888 0.63720612 0.24397396 0.8229143\n",
      " 0.46230706 0.86802147 0.38494523 0.13139873 0.13552027 0.530692\n",
      " 0.93553848 0.84921616 0.50460379 0.92955103 0.94162336 0.87367299\n",
      " 0.59109049 0.99382001 0.04041932 0.58510769 0.76853065 0.13216486\n",
      " 0.7274602  0.01800727 0.36333133 0.25597232 0.37993438 0.23081752\n",
      " 0.8313985  0.65798556 0.9050858  0.18543082 0.56512354 0.78065603\n",
      " 0.135969   0.82839559 0.55959034 0.65445182 0.44192195 0.69287971\n",
      " 0.07023098 0.59419945 0.16921773 0.55303354 0.10188355 0.40957567\n",
      " 0.05925513 0.18901446 0.78943052 0.29538669 0.84168455 0.74767331\n",
      " 0.9701807  0.01804574 0.56191877 0.0412335  0.02667463 0.11241227\n",
      " 0.19966632 0.69739505 0.92510573 0.57241619 0.75977462 0.5846637\n",
      " 0.3141283  0.46937446 0.28995043 0.09053758 0.2724904  0.93218602\n",
      " 0.24242371 0.46540782 0.15688075 0.24612203 0.42591294 0.57781089\n",
      " 0.24801826 0.03122868 0.92550712 0.44518601 0.24171792 0.13834351\n",
      " 0.79287648 0.52768661 0.67386237 0.96868301 0.80428505 0.52683248\n",
      " 0.91206289 0.41379241 0.47088415 0.53161505 0.84406503 0.78972414\n",
      " 0.92079684 0.05300685 0.78239262 0.70891246 0.61451584 0.13190199\n",
      " 0.63434422 0.93690651 0.55788412 0.38312638 0.21406486 0.99603999\n",
      " 0.87364698 0.74982357 0.56147233 0.49453561 0.80577685 0.14299362\n",
      " 0.95408024 0.35297066 0.46763987 0.87067768 0.09123153 0.95195002\n",
      " 0.20533214 0.29313814 0.879038   0.35856247 0.42085258 0.96232581\n",
      " 0.60612839 0.07746757 0.18602797 0.99381968 0.67441664 0.54184239\n",
      " 0.25499727 0.45527208 0.96161181 0.76637606 0.77755043 0.20541097\n",
      " 0.50695618 0.94042749 0.43657175 0.53773807 0.43107003 0.56781451\n",
      " 0.55427106 0.07188461 0.74881066 0.18995984 0.69006755 0.86941108\n",
      " 0.41935587 0.14968113 0.10747362 0.37733227 0.81585482 0.25097437\n",
      " 0.27733301 0.96332448 0.82134724 0.74426563 0.74426856 0.25941175\n",
      " 0.27766265 0.32408258 0.03395507 0.12708606 0.47862878 0.70310708\n",
      " 0.15237404 0.08242483 0.2540858  0.02153051 0.67393694 0.47467852\n",
      " 0.47127868 0.49278498 0.43606467 0.84353278 0.66879561 0.62261604\n",
      " 0.97988996 0.6919486  0.06583344 0.5499264  0.25424149 0.99245765\n",
      " 0.87272442 0.22133922 0.22034821 0.70329796 0.67877143 0.7782645\n",
      " 0.35666559 0.48724362 0.95547559 0.98893883 0.59379756 0.43893836\n",
      " 0.11961768 0.76481512 0.52937591 0.32204299 0.60375575 0.34343263\n",
      " 0.07452422 0.0772471  0.54907174 0.4417089  0.18416022 0.66167122\n",
      " 0.17965402 0.32717675 0.03183252 0.15664884 0.25368768 0.78388477\n",
      " 0.79055141 0.30402321 0.05010845 0.59071386 0.50656827 0.52620057\n",
      " 0.57989342 0.22688781 0.23631914 0.76565372 0.88865252 0.31676252\n",
      " 0.17871379 0.08542583 0.10082293 0.47790908 0.30733853 0.93043161\n",
      " 0.15934964 0.1249726  0.84004263 0.44036179 0.11873194 0.88769099\n",
      " 0.07111655 0.63975312 0.94469667 0.15315379 0.53774755 0.72681564\n",
      " 0.8384938  0.09438043 0.0996608  0.9607528  0.96900302 0.58544865\n",
      " 0.39395678 0.26432632 0.43249443 0.11337961 0.62498838 0.33808761\n",
      " 0.14854469 0.96905591 0.85137602 0.70547799 0.50978016 0.32027927\n",
      " 0.64223149 0.39885306 0.27568944 0.76656037 0.79098233 0.51179066\n",
      " 0.29151843 0.25473054 0.38167985 0.4504467  0.55739733 0.2529081\n",
      " 0.94036742 0.16990936 0.68587997 0.47379955 0.20933659 0.17744229\n",
      " 0.78513184 0.94820965 0.8518693  0.49698118 0.17547028 0.07349742\n",
      " 0.464224   0.68393372 0.79423867 0.35522122 0.91104902 0.03433052\n",
      " 0.93715753 0.97031623 0.04582268 0.39445389 0.76564254 0.43561929\n",
      " 0.81147378 0.20854575 0.52564933 0.63283891 0.40349759 0.90768489\n",
      " 0.66976868 0.90249032 0.32630468 0.8917482  0.46080832 0.69250652\n",
      " 0.08328035 0.53296291 0.18206654 0.68159922 0.70354453 0.38562887\n",
      " 0.4030735  0.58476285 0.08041145 0.89303187 0.53776363 0.30219028\n",
      " 0.56775817 0.94921737 0.49223833 0.89660558 0.37805323 0.02652899\n",
      " 0.69871618 0.01759876 0.21913021 0.93940576 0.97589089 0.1971876\n",
      " 0.89874522 0.12232052 0.45804869 0.12085476 0.60840195 0.19129611\n",
      " 0.12379335 0.40219343 0.03646916 0.96996414 0.430912   0.71036889\n",
      " 0.0429247  0.27064177 0.46183959 0.53959746 0.38504363 0.18296382\n",
      " 0.37777483 0.14976435 0.22956914 0.63536596 0.64704554 0.99253111\n",
      " 0.86637489 0.40143911 0.89202672 0.47888108 0.66632203 0.91807406\n",
      " 0.87074211 0.73193495 0.64660004 0.51913893 0.28271577 0.53661849\n",
      " 0.97950292 0.6916415  0.54370875 0.94866916 0.8586762  0.87341001\n",
      " 0.18643486 0.58183592 0.21104635 0.90507676 0.78765132 0.65666955\n",
      " 0.83163599 0.17580322 0.02214259 0.98986235 0.28768248 0.37418705\n",
      " 0.15265781 0.79067314 0.32967519 0.7165078  0.19130369 0.06720982\n",
      " 0.54393    0.94968216 0.89691699 0.55400407 0.68709351 0.43766963\n",
      " 0.4225078  0.54764233 0.52213387 0.44330247 0.17255341 0.08152211\n",
      " 0.88664583 0.70970395 0.14962334 0.08422931 0.63497282 0.64381434\n",
      " 0.03802121 0.70907754 0.98818493 0.93070874 0.53339035 0.67100964\n",
      " 0.84585985 0.68650477 0.48408027 0.68597579 0.39487866 0.96997554\n",
      " 0.77344613 0.28010887 0.06391758 0.12912261 0.23616793 0.3595629\n",
      " 0.52886859 0.51438914 0.70521731 0.09035926 0.72499771 0.24983006\n",
      " 0.23592777 0.51127802 0.36726861 0.41247768 0.7979727  0.40821258\n",
      " 0.57289748 0.57137421 0.03357594 0.00115612 0.9822834  0.55931287\n",
      " 0.43584919 0.1262327  0.69418345 0.40457633 0.02835572 0.25620476\n",
      " 0.61505466 0.83317753 0.84735322 0.50557401 0.68213133 0.33866241\n",
      " 0.29771723 0.49329536 0.4072724  0.29822875 0.21200943 0.23711223\n",
      " 0.49859293 0.95483622 0.05363719 0.92723849 0.78761631 0.23834085\n",
      " 0.88669752 0.96371769 0.74087576 0.52919374 0.15236733 0.74185879\n",
      " 0.17214928 0.4553668  0.97961291 0.08232463 0.85802608 0.68505621\n",
      " 0.99882535 0.56855597 0.17926618 0.69158074 0.32926448 0.13507509\n",
      " 0.37321497 0.15776262 0.88637733 0.95802114 0.5671893  0.49745773\n",
      " 0.00663396 0.93563151 0.11902872 0.66162652 0.72694424 0.65182647\n",
      " 0.46609648 0.74922778 0.20424398 0.83263842]\n"
     ]
    }
   ],
   "source": [
    "x1 = np.concatenate((classes[0],classes[1]))\n",
    "y1 = np.concatenate((labels[0],labels[1]))\n",
    "# for i in range(2000):\n",
    "#     t,b = grad_des(x1,y1,theta1,bias1)\n",
    "#     theta1 = t\n",
    "#     bias1 = b\n",
    "    \n",
    "print(theta1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(x1,y1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest = x1[0,:].reshape(1,-1)\n",
    "lr.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for labels 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.concatenate((classes[1],classes[2]))\n",
    "y1 = np.concatenate((labels[0],labels[1]))\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x1,y1)\n",
    "lr.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for labels 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.concatenate((classes[2],classes[3]))\n",
    "y1 = np.concatenate((labels[0],labels[1]))\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x1,y1)\n",
    "lr.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for labels 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.concatenate((classes[3],classes[4]))\n",
    "y1 = np.concatenate((labels[0],labels[1]))\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x1,y1)\n",
    "lr.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
